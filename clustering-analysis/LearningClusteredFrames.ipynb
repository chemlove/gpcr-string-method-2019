{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T09:47:47.873053Z",
     "start_time": "2018-11-26T09:47:47.840408Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "#imports\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s %(name)s-%(levelname)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import scipy.ndimage.filters\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sys.path.append('MD_common/')\n",
    "sys.path.append('heatmapping/')\n",
    "#sys.path.append('/home/oliverfl/git/interprettensor/interprettensor/modules/')\n",
    "from helpfunc import *\n",
    "from colvars import *\n",
    "import nbimporter\n",
    "import AnalyzeClusteredFrames as ancf\n",
    "import MD_fun\n",
    "import modules, utils\n",
    "from trajclassifier import *\n",
    "from relevancepropagator import *\n",
    "\n",
    "fun = MD_fun.MD_functions()\n",
    "os.chdir(get_project_path())\n",
    "#simulations = [(\"A\", \"08\"), (\"A\", \"00\")]  #, (\"A\", \"16\")]\n",
    "logger = logging.getLogger(\"learnclust\")\n",
    "traj_type = \"drorA_3_clusters\"#\"strings_apo_holo\"#\"drorD\",\"freemd_apo\"\n",
    "distance_metric =\"distance_closest-heavy\"#\"contact_closest-heavy\" #\"CA\" #cvs-len5, CA, CAonlyCvAtoms, distance_closest-heavy\n",
    "cvs_name = \"cvs-{}\".format(traj_type)\n",
    "CA_query = None\n",
    "logger.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load clustering data from other module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T09:46:58.632459Z",
     "start_time": "2018-11-26T09:46:58.228976Z"
    }
   },
   "outputs": [],
   "source": [
    "nclusters = int(traj_type.split(\"_\")[1])\n",
    "cluster_simu = Simulation({\n",
    "    \"condition\": \"A\",\n",
    "    \"number\": \"00\",\n",
    "    \"name\": \"all\",\n",
    "    \"stride\": 100\n",
    "})\n",
    "cluster_simu.clusterpath=\"Result_Data/beta2-dror/clustering/\"\n",
    "cluster_simu = ancf.load_default(cluster_simu)\n",
    "clustering_id = \"drorA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distance metric, e.g. CA distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T09:55:43.251301Z",
     "start_time": "2018-11-26T09:55:24.195905Z"
    }
   },
   "outputs": [],
   "source": [
    "if distance_metric.startswith(\"contact\") or distance_metric.startswith(\"distance\"):\n",
    "    scheme = distance_metric.split(\"_\")[-1]\n",
    "    logger.debug(\"Using scheme %s for computing distance metric %s\", scheme, distance_metric)\n",
    "    atoms = get_atoms(\"protein and name CA\",cluster_simu.traj.top, sort=False)\n",
    "    CA_atoms, cv_atoms = atoms, None\n",
    "    protein_residues = [a.residue.index for a in atoms]\n",
    "    protein_residues = sorted(protein_residues)\n",
    "    frame_distances = np.zeros((len(cluster_simu.traj), len(protein_residues), len(protein_residues)))\n",
    "    cutoff = 0.5\n",
    "    for idx, r1 in enumerate(protein_residues):\n",
    "        if idx == len(protein_residues) - 1:\n",
    "            break\n",
    "        if idx % 10 == 0:\n",
    "            logger.debug(\"Computing contacts for residue %s/%s\", idx + 1, len(protein_residues))\n",
    "        res_pairs = [(r1,r2) for r2 in protein_residues[idx+1:]]\n",
    "        dists, dist_atoms = md.compute_contacts(cluster_simu.traj,\n",
    "                                               contacts=res_pairs,\n",
    "                                               scheme=scheme,\n",
    "                                               ignore_nonprotein=True)\n",
    "        if distance_metric.startswith(\"contact\"):\n",
    "            contacts  = dists\n",
    "            contacts[contacts > cutoff] = 0\n",
    "            contacts[contacts > 0] = 1\n",
    "            frame_distances[:,idx,(idx+1):] = contacts\n",
    "            frame_distances[:,(idx+1):,idx] = contacts    \n",
    "        elif distance_metric.startswith(\"distance\"):\n",
    "            inv_dists = 1/dists\n",
    "            frame_distances[:,idx,(idx+1):] = inv_dists\n",
    "            frame_distances[:,(idx+1):,idx] = inv_dists    \n",
    "elif distance_metric.startswith(\"cvs\"):\n",
    "    cvs = load_object(\"cvs/\" + distance_metric)\n",
    "    frame_distances = eval_cvs(cluster_simu.traj, cvs)\n",
    "    CA_atoms = None\n",
    "    CA_query=None\n",
    "    cv_atoms = []\n",
    "    for idx, cv in enumerate(cvs):\n",
    "        resq = \"name CA and (resSeq {} or resSeq {})\".format(cv.res1, cv.res2) \n",
    "        res_atoms =  get_atoms(resq, cluster_simu.traj.topology, sort=False)\n",
    "        cv_atoms.append(tuple(res_atoms))\n",
    "    logger.debug(cv_atoms)       \n",
    "else:\n",
    "    raise Exception(\"Unsupported value \" + distance_metric)\n",
    "    \n",
    "logger.debug(\"Done. Loaded distances into a matrix of shape %s\",\n",
    "         frame_distances.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-13T08:28:08.788779Z",
     "start_time": "2017-09-13T08:28:08.784106Z"
    }
   },
   "source": [
    "# Train Network\n",
    "- Using http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T09:56:15.102912Z",
     "start_time": "2018-11-26T09:55:43.365197Z"
    }
   },
   "outputs": [],
   "source": [
    "trainingstep = 2 #set to something else to test prediction power\n",
    "####Optionally shuffle indices:\n",
    "#indices = np.arange(Äºen(cluster_simu.traj))\n",
    "#np.random.shuffle(indices)\n",
    "#frame_distances = frame_distances[indices]\n",
    "#cluster_simu.traj = cluster_simu.traj[indices]\n",
    "training_samples, target_values, scaler, classifier = transform_and_train(\n",
    "    frame_distances, cluster_simu, trainingstep=trainingstep)\n",
    "logger.debug(\"Done with learning (trainingstep=%s)\", trainingstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T09:56:15.473114Z",
     "start_time": "2018-11-26T09:56:15.204229Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_predictions(\n",
    "    classifier.predict(training_samples), training_samples, target_values)\n",
    "logger.debug(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Layer-Wise Relevance Propagation \n",
    "* **relevance propagation method** described at http://heatmapping.org/tutorial/\n",
    "\n",
    "* **Some info on MLP** (from https://www.hiit.fi/u/ahonkela/dippa/node41.html):\n",
    "\n",
    "The computations performed by such a feedforward network with a single hidden layer with nonlinear activation functions and a linear output layer can be written mathematically as\n",
    "\n",
    " $\\displaystyle \\mathbf{x}= \\mathbf{f}(\\mathbf{s}) = \\mathbf{B}\\boldsymbol{\\varphi}( \\mathbf{A}\\mathbf{s}+ \\mathbf{a} ) + \\mathbf{b}$\t(4.15)\n",
    "\n",
    "where  $ \\mathbf{s}$ is a vector of inputs and  $ \\mathbf{x}$ a vector of outputs.  $ \\mathbf{A}$ is the matrix of weights of the first layer,  $ \\mathbf{a}$ is the bias vector of the first layer.  $ \\mathbf{B}$ and  $ \\mathbf{b}$ are, respectively, the weight matrix and the bias vector of the second layer. The function  $ \\boldsymbol{\\varphi}$ denotes an elementwise nonlinearity. The generalisation of the model to more hidden layers is obvious.\n",
    "\n",
    "* **About the MLP implementation we use**:\n",
    "\n",
    "If you do want to extract the MLP weights and biases after training your model, you use its public attributes coefs_ and intercepts_.\n",
    "- coefs_ is a list of weight matrices, where weight matrix at index i represents the weights between layer i and layer i+1.\n",
    "- intercepts_ is a list of bias vectors, where the vector at index i represents the bias values added to layer i+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T09:56:17.323443Z",
     "start_time": "2018-11-26T09:56:15.576307Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = classifier.coefs_\n",
    "biases = classifier.intercepts_\n",
    "\n",
    "propagation_samples = training_samples\n",
    "propagation_values = target_values\n",
    "#Using cluster reps gives good results\n",
    "# propagation_samples = training_samples[cluster_simu.cluster_rep_indices]\n",
    "# propagation_values = target_values[cluster_simu.cluster_rep_indices]\n",
    "\n",
    "relevance = relevance_propagation(weights, biases, propagation_samples,\n",
    "                                  propagation_values)\n",
    "sensitivity = sensitivity_analysis(weights, biases, propagation_samples,\n",
    "                                   propagation_values)\n",
    "logger.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the relevance propagation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T09:56:18.661151Z",
     "start_time": "2018-11-26T09:56:17.413155Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_relevance, avg_sensitivity = analyze_relevance(relevance, sensitivity,\n",
    "                                                   target_values, plot=True, max_scale=True)\n",
    "# relevance_per_cluster, sensitivity_per_cluster = analyze_relevance_per_cluster(relevance, sensitivity, target_values)\n",
    "# relevance_per_cluster, sensitivity_per_cluster = analyze_relevance_per_cluster(relevance, sensitivity, target_values)\n",
    "def to_atom_pairs(avg_relevance, avg_sensitivity, rowcount, atoms):\n",
    "    \"\"\"Convert the avg relevance and sensitivity to AtomPair. Instead of real distance we use the relevance\"\"\"\n",
    "    #Convert to\n",
    "    nfeatures = len(avg_relevance)\n",
    "    pairs = np.empty((nfeatures, ), dtype=object)\n",
    "    feature_to_resids = np.empty((nfeatures,2), dtype=int)\n",
    "    for idx, rel in enumerate(avg_relevance):\n",
    "        if cv_atoms is not None:\n",
    "            atom1, atom2 = cv_atoms[idx]\n",
    "        else:\n",
    "            atomidx1, atomidx2 = to_matrix_indices(idx, rowcount)\n",
    "            atom1, atom2 = atoms[atomidx1], atoms[atomidx2]\n",
    "        pair = ancf.AtomPair(rel, atom1, atom2)\n",
    "        pair.relevance = rel\n",
    "        pair.sensitivity = avg_sensitivity[idx]\n",
    "        pairs[idx] = pair\n",
    "        feature_to_resids[idx, 0] = atom1.residue.resSeq\n",
    "        feature_to_resids[idx, 1] = atom2.residue.resSeq\n",
    "    return pairs, feature_to_resids\n",
    "atom_pairs, feature_to_resids = to_atom_pairs(avg_relevance, avg_sensitivity, frame_distances.shape[1], CA_atoms)\n",
    "logger.debug(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVs evaluation\n",
    "## Picking those with highest relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T09:56:51.187244Z",
     "start_time": "2018-11-26T09:56:50.114381Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvs = []\n",
    "cutoff = 0.98 #0.8\n",
    "cvs_definition = []\n",
    "for ap in sorted(atom_pairs, cmp=lambda ap1,ap2 : -1 if ap1.relevance[0,0] > ap2.relevance[0,0] else 1):\n",
    "    rel = ap.relevance[0,0]\n",
    "    if rel < cutoff:\n",
    "        break    \n",
    "    a1,a2 = ap.atom1, ap.atom2\n",
    "    #print(a1,a2, relevance)\n",
    "    cvid = \"{}-{}\".format(a1.residue,a2.residue)\n",
    "    res1, res2 = a1.residue.resSeq, a2.residue.resSeq\n",
    "    cv = CADistanceCv(cvid, res1, res2, periodic=True)\n",
    "    logger.debug(\"%s has relevance %s\",cv, rel)\n",
    "    cv.normalize(trajs=[cluster_simu.traj])\n",
    "    cvs.append(cv)\n",
    "    cvs_definition.append({\"@class\":\"CADistanceCv\", \"periodic\": True, \"id\":cvid, \"res1\":res1, \"res2\":res2, \"scale\":cv._norm_scale+0,\"offset\": cv._norm_offset+0})\n",
    "\n",
    "logger.debug(\"%s CVs in total\", len(cvs))\n",
    "\n",
    "def to_vmd_query(ca_cvs):\n",
    "    allRes = \" \".join([\"{} {}\".format(cv.res1, cv.res2) for cv in ca_cvs])\n",
    "    vmdq = \"name CA and resid {}\".format(allRes)\n",
    "    return vmdq\n",
    "logger.debug(\"VMD query for plotting CAs:\\n%s\", to_vmd_query(cvs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T14:54:07.551487Z",
     "start_time": "2018-09-05T14:54:07.541449Z"
    }
   },
   "outputs": [],
   "source": [
    "json_filename = \"cvs-%s-len%s\"%(traj_type, len(cvs))\n",
    "logger.info(\"Saving CVs to file %s\", json_filename)\n",
    "persist_object(cvs, json_filename)\n",
    "with open(\"cvs/\" + json_filename + \".json\", 'w') as fp:\n",
    "    json.dump({\"cvs\": cvs_definition},fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T08:28:47.907537Z",
     "start_time": "2018-10-05T08:28:47.697918Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cluster_indices = np.array(cluster_simu.cluster_indices)\n",
    "median_cluster_vals =np.empty((nclusters, len(cvs)))\n",
    "order_to_cluster = [2,1,3]\n",
    "for cid, cv in enumerate(cvs):\n",
    "    evals = cv.eval(cluster_simu.traj)\n",
    "    plt.plot(evals, '--', alpha=0.25, label=cv.id)\n",
    "    for c in range(nclusters):\n",
    "        c_indices = np.argwhere(cluster_indices == order_to_cluster[c])\n",
    "        median_cluster_vals[c,cid] = np.median(evals[c_indices])\n",
    "np.savetxt(\"stringpath-cluster-median-{}-len{}.txt\".format(traj_type, len(cvs)), median_cluster_vals)\n",
    "if len(cvs) < 20:\n",
    "    plt.legend(loc=(1.02,0))\n",
    "plt.show()\n",
    "#to_vmd_query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CVS with partion graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-30T13:05:32.753191Z",
     "start_time": "2018-08-30T13:05:30.954022Z"
    }
   },
   "outputs": [],
   "source": [
    "def partition_as_graph(atom_pairs,\n",
    "                       dist_func=lambda p: p.relevance,\n",
    "                       percentile=99.95,\n",
    "                       explain=True,\n",
    "                      max_partition_count=30):\n",
    "    final_distances = np.array([dist_func(p) for p in atom_pairs])\n",
    "    cutoff = np.percentile(final_distances, percentile)\n",
    "    graph = ancf.partition_as_graph(\n",
    "        atom_pairs,\n",
    "        dist_func=dist_func,\n",
    "        cutoff=cutoff,\n",
    "        split_subgraphs=True,\n",
    "        max_partition_count=max_partition_count)\n",
    "    if explain:\n",
    "        graph.explain_to_human()\n",
    "    return graph\n",
    "\n",
    "\n",
    "atom_pairs = to_atom_pairs(avg_relevance, avg_sensitivity, frame_distances.shape[1], CA_atoms)\n",
    "logger.info(\"Partitioning atom pairs to a colored graph\")\n",
    "percentile=99.5\n",
    "graph = partition_as_graph(\n",
    "    atom_pairs, dist_func=lambda p: p.relevance, percentile=percentile)\n",
    "logger.debug(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Plot the CVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-24T11:56:04.308251Z",
     "start_time": "2018-08-24T11:55:48.082462Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_generator, id_generator = ancf.most_relevant_dist_generator(graph, atom_pairs)\n",
    "cvs = ancf.create_cvs(graph, CV_generator=cv_generator, ID_generator=id_generator)\n",
    "sys.setrecursionlimit(10000) #might be necssary for Pickle...\n",
    "# cvs = ancf.create_cvs(graph, CV_generator=ancf.compute_color_mean_distance)\n",
    "# cvs = ancf.create_cvs(graph, CV_generator=ancf.compute_color_center_distance)\n",
    "cvs = normalize_cvs(cvs, simulations=[cluster_simu])\n",
    "cvs_filename = \"cvs-len%s\"%(len(cvs))\n",
    "logger.info(\"Saving CVs to file %s\", cvs_filename)\n",
    "persist_object(cvs, cvs_filename)\n",
    "logger.info(\"#distances as input=%s, percentile=%s, graph of %s atoms and %s colors -> %s distance CVs\", \n",
    "            len(graph.atompairs), percentile, len(graph.nodes), len(graph.colors), len(cvs))\n",
    "\n",
    "ancf.create_cluster_plots(cluster_simu, atom_pairs, graph, cvs)\n",
    "logger.debug(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plot order parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-03T09:28:12.680133Z",
     "start_time": "2018-07-03T09:28:12.433188Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(cluster_simu.cluster_indices,'--', label=\"Cluster state\", alpha=0.3)\n",
    "graph.plot_distances(cluster_simu, histogram=True, separate_clusters=False, max_per_plot=10, bincount=10, use_contacts=False)\n",
    "graph.plot_distances(cluster_simu, histogram=False, separate_clusters=False, max_per_plot=10, bincount=10, use_contacts=False)\n",
    "logger.debug(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank the atoms with most relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-05T10:03:40.731741Z",
     "start_time": "2018-10-05T10:03:40.605236Z"
    }
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Iterable \n",
    "\n",
    "relevance_cutoff = 0.3 \n",
    "\n",
    "def compute_relevance_per_atom_for_pairs(atom_pairs):\n",
    "    def add_relevance(relevance_count, atom, relevance):\n",
    "        if relevance > relevance_cutoff: #get rid of noise \n",
    "            relevance_count[atom] = relevance_count.get(atom, 0) + relevance\n",
    "\n",
    "    relevance_count = {}\n",
    "    for ap in atom_pairs:\n",
    "        add_relevance(relevance_count, ap.atom1, ap.relevance[0])\n",
    "        add_relevance(relevance_count, ap.atom2, ap.relevance[0])\n",
    "    return [\n",
    "        (k, v[0, 0])\n",
    "        for k, v in sorted(\n",
    "            relevance_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    ]\n",
    "\n",
    "def compute_relevance_per_atom_for_coordinates(atom_pairs):\n",
    "    relevance_count = []\n",
    "    #Merge relevance per residue\n",
    "    resSeq_to_CA_relevance = {\n",
    "        \n",
    "    }\n",
    "    #Merge XYZ for atoms\n",
    "    for idx, a in enumerate(all_atoms):\n",
    "        rels = avg_relevance[3*idx:3*(idx+1)]\n",
    "        #Average coordiantes\n",
    "        atom_rel = rels[rels > relevance_cutoff].sum()\n",
    "        resSeq = a.residue.resSeq\n",
    "        current_atom, current_rel = resSeq_to_CA_relevance.get(resSeq, (a,0.))\n",
    "        if a.name == \"CA\":\n",
    "            current_atom = a\n",
    "        current_rel += atom_rel\n",
    "        resSeq_to_CA_relevance[resSeq] = (current_atom, current_rel)\n",
    "    relevance_count = [\n",
    "        (k, v)\n",
    "        for k, v in sorted(\n",
    "            resSeq_to_CA_relevance.values(), key=operator.itemgetter(1), reverse=True)\n",
    "    ]\n",
    "    #plt.hist([r for (a,r) in relevance_count])\n",
    "    #plt.show()\n",
    "    return relevance_count\n",
    "\n",
    "def compute_relevance_per_atom(atoms):\n",
    "    if len(atoms) == 0:\n",
    "        return []\n",
    "    if isinstance(atoms[0], Iterable):\n",
    "        return compute_relevance_per_atom_for_pairs(atoms)\n",
    "    else:\n",
    "        return compute_relevance_per_atom_for_coordinates(atoms)\n",
    "    \n",
    "def to_full_vmd_beta_script(relevance_count):\n",
    "    max_rel = relevance_count[0][1]\n",
    "    min_rel = relevance_count[len(relevance_count)-1][1]\n",
    "    script = \"\";\n",
    "    for a,r in relevance_count:\n",
    "        beta = 10*(r - min_rel)/(max_rel-min_rel)\n",
    "        script += to_vmd_beta_value(a.residue.resSeq, beta)\n",
    "    return script\n",
    "    \n",
    "relevance_count = compute_relevance_per_atom(all_atoms if distance_metric == \"coordinates\" else atom_pairs)\n",
    "max_to_print = 10\n",
    "for i, (a, r) in enumerate(relevance_count):\n",
    "    if i >= max_to_print:\n",
    "        break\n",
    "    logger.info(\"Atom %s has relevance %s\", a, r)\n",
    "vmd_beta_script = to_full_vmd_beta_script(relevance_count)\n",
    "#Print vmd_beta_script and paste into TK-console. Set color to beta\n",
    "#logger.debug(\"Command to color protein residues in VMD:\\n%s\", vmd_beta_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classifier CVs\n",
    "\n",
    "create CVs you can use such as: \n",
    "\n",
    "```python\n",
    "evals = cv.eval(traj)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T15:01:58.451348Z",
     "start_time": "2018-10-24T15:01:58.446375Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discrete_classifier_cv, probaility_classifier_cvs = create_classifier_cvs(clustering_id, training_samples, target_values, scaler, classifier, trainingstep, query=CA_query, cvs=cvs)\n",
    "logger.debug(\"Created CVs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save classifier CVs to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T09:57:45.730177Z",
     "start_time": "2018-11-26T09:57:37.917097Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_data(save_dir, frame_distances, training_samples, target_values, feature_to_resids):\n",
    "    np.save(\"{}/frame_distances\".format(save_dir),frame_distances)\n",
    "    np.save(\"{}/training_samples\".format(save_dir),training_samples)\n",
    "    np.save(\"{}/target_values\".format(save_dir), target_values)\n",
    "    np.save(\"{}/feature_to_resids\".format(save_dir), feature_to_resids) \n",
    "    \n",
    "def save_sklearn_objects(save_dir, scaler, classifier):\n",
    "    persist_object(classifier, \"{}/classifier\".format(save_dir))\n",
    "    persist_object(scaler, \"{}/scaler\".format(save_dir))\n",
    "\n",
    "def save_cvs(save_dir, discrete_classifier_cv, probaility_classifier_cvs):\n",
    "    persist_object(discrete_classifier_cv, \"{}/discrete_classifier_cv\".format(save_dir))\n",
    "    persist_object(probaility_classifier_cvs, \"{}/probability_classifier_cvs\".format(save_dir))\n",
    "    \n",
    "save_dir = \"neural_networks/\" + clustering_id + \"-\" + distance_metric \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_data(save_dir, frame_distances, training_samples, target_values, feature_to_resids)\n",
    "save_sklearn_objects(save_dir, scaler, classifier)\n",
    "save_cvs(save_dir, discrete_classifier_cv, probaility_classifier_cvs)\n",
    "\n",
    "logger.debug(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1142px",
    "left": "0px",
    "right": "20px",
    "top": "107px",
    "width": "273px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "645px",
    "left": "900px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
